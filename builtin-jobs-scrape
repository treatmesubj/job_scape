#!/usr/bin/env python3
import os
import subprocess
import json
import requests
from bs4 import BeautifulSoup
import browser_cookie3
import glob
import argparse
import time


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--jobs",
        "-j",
        action="store",
        type=int,
        default=20,
        help="int, number of jobs to scrape",
    )
    parser.add_argument(
        "--sleeptime",
        "-s",
        action="store",
        type=int,
        default=60,
        help="int, number of seconds to sleep between each job-scrape",
    )
    parser.add_argument(
        "--outset",
        "-o",
        action="store",
        type=int,
        default=0,
        help="int, job index to start scraping at",
    )
    args = parser.parse_args()

    # cookiejar = browser_cookie3.load()  # all browsers
    # if WSL, use Windows Firefox cookies
    kernel = subprocess.check_output(["uname", "-a"]).decode().strip().lower()
    if any(str in kernel for str in ["microsoft", "wsl"]):
        user = subprocess.check_output(["powershell.exe", "$Env:UserName"]).decode().strip()
        ff_profile = browser_cookie3.FirefoxBased.get_default_profile(
            f"/mnt/c/Users/{user}/AppData/Roaming/Mozilla/Firefox"
        )
        cookie_file = glob.glob(os.path.join(ff_profile, "cookies.sqlite"))[0]
        cookiejar = browser_cookie3.firefox(cookie_file=cookie_file)
    else:
        cookiejar = browser_cookie3.firefox()

    job_num = args.outset
    page_num = int(args.outset / 10)
    while job_num < (args.outset + args.jobs):
        response_jobs = requests.get(
            f"https://builtin.com/jobs/remote&page={page_num}",
            cookies=cookiejar,
            allow_redirects=True,
        )

        soup = BeautifulSoup(response_jobs.text, "html.parser")

        job_post_elems = soup.select("div[id^='job-card']")

        for elem in job_post_elems:
            try:
                assert job_num < (args.outset + args.jobs), "done"
            except AssertionError:
                break
            company_name = elem.select_one("div[data-id] span").text
            job_title = elem.select_one("[href^='/job/']").text
            builtin_apply_url = (
                "https://builtin.com" + elem.select_one("[href^='/job/']")["href"]
            )

            response_builtin_job = requests.get(
                builtin_apply_url,
                cookies=cookiejar,
                allow_redirects=True,
            )

            soup = BeautifulSoup(response_builtin_job.text, "html.parser")
            try:
                job_apply_url = soup.select_one(
                    "div[class*='apply-job-form'] div[data-path]"
                )["data-path"]
            except Exception as e:
                print(
                    "Built In is likely rate-limiting you; courteously try sleeping for 60 seconds between scrapes"
                )
                raise e

            post_csv = f'"{company_name}","{job_title}","{job_apply_url}","{builtin_apply_url}"'
            print(f"[{job_num}]: {post_csv}")

            os.makedirs(os.path.dirname("./data/"), exist_ok=True)
            with open("./data/builtin-jobs.csv", "a", encoding="utf-8") as f:
                f.write(f"{post_csv}\n")

            job_num += 1
            time.sleep(args.sleeptime)

        page_num += 1
