#!/usr/bin/env python3
import os
import subprocess
import json
import requests
from bs4 import BeautifulSoup
import browser_cookie3
import glob
import argparse
import time


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--jobs",
        "-j",
        action="store",
        type=int,
        default=20,
        help="int, number of jobs to scrape",
    )
    parser.add_argument(
        "--remote-collection",
        "-rc",
        action="store_true",
        help="results from LinkedIn remote jobs collection",
    )
    parser.add_argument(
        "--sleeptime",
        "-s",
        action="store",
        type=int,
        default=0,
        help="int, number of seconds to sleep between each job-scrape",
    )
    parser.add_argument(
        "--outset",
        "-o",
        action="store",
        type=int,
        default=0,
        help="int, job index to start scraping at",
    )
    args = parser.parse_args()

    # cookiejar = browser_cookie3.load()  # all browsers
    # if WSL, use Windows Firefox cookies
    kernel = subprocess.check_output(["uname", "-a"]).decode().strip().lower()
    if any(str in kernel for str in ["microsoft", "wsl"]):
        user = subprocess.check_output(["powershell.exe", "$Env:UserName"]).decode().strip()
        ff_profile = browser_cookie3.FirefoxBased.get_default_profile(
            f"/mnt/c/Users/{user}/AppData/Roaming/Mozilla/Firefox"
        )
        cookie_file = glob.glob(os.path.join(ff_profile, "cookies.sqlite"))[0]
        cookiejar = browser_cookie3.firefox(cookie_file=cookie_file)
    else:
        cookiejar = browser_cookie3.firefox()

    if not os.path.exists('./data/linkedin-jobs.csv'):
        os.makedirs(os.path.dirname("./data/"), exist_ok=True)
        with open("./data/linkedin-jobs.csv", "a", encoding="utf-8") as f:
            f.write(f"company_name, job_title, job_apply_url, job_id_url\n")

    if args.remote_collection:
        job_list_url = "https://www.linkedin.com/jobs/collections/remote-jobs/?"
    else:
        job_list_url = "https://www.linkedin.com/jobs/search/?keywords=&location=United States&f_TPR=&f_WT=2&"

    job_num = args.outset
    while job_num < (args.outset + args.jobs):
        response_jobs = requests.get(
            f"{job_list_url}start={job_num}",
            cookies=cookiejar,
            allow_redirects=True,
        )

        soup = BeautifulSoup(response_jobs.text, "html.parser")

        jobs_elem = soup.select("[id^='bpr-guid']")[-1]

        sanjay = json.loads(jobs_elem.text)
        if args.remote_collection:
            job_keys = [
                key
                for key in sanjay['data']['data']['jobsDashJobCardsByJobCollections']['metadata']["jobCardPrefetchQueries"][0][
                    "*prefetchJobPostingCard"
                ]
            ]
        else:
            job_keys = [
                key
                for key in sanjay["data"]["metadata"]["jobCardPrefetchQueries"][0][
                    "prefetchJobPostingCard"
                ].keys()
            ]
        job_ids = [key.split("(")[-1].split(",")[0] for key in job_keys]

        for id in job_ids:
            try:
                assert job_num < (args.outset + args.jobs), "done"
            except AssertionError:
                break
            job_id_url = f"https://www.linkedin.com/jobs/search/?currentJobId={id}"
            response_job = requests.get(
                job_id_url,
                cookies=cookiejar,
                allow_redirects=True,
            )

            soup = BeautifulSoup(response_job.text, "html.parser")
            job_post_elems = soup.select("[id^='bpr-guid']")

            # apply URL
            for elem in job_post_elems:
                try:
                    sanjay = json.loads(elem.text)
                    job_apply_method = sanjay["data"]["applyMethod"]
                    break
                except KeyError:
                    pass
            else:
                # raise Exception(f"{id}, no apply method found")
                print(f"{id}, no apply method found")
                continue  # just go to next job
            if "companyApplyUrl" in job_apply_method.keys():
                job_apply_url = job_apply_method["companyApplyUrl"]  # company site
            elif "easyApplyUrl" in job_apply_method.keys():  # LinkedIn Easy-Apply
                job_apply_url = job_id_url
            elif (
                "SimpleOnsiteApply" in job_apply_method["$type"]
            ):  # LinkedIn super Easy-Apply
                job_apply_url = job_id_url
            else:
                raise Exception(f"{job_id_url}, no apply URL?")

            # job title
            job_title = sanjay["data"]["title"]

            # company name
            if "companyName" in sanjay["data"]["companyDetails"].keys():
                company_name = sanjay["data"]["companyDetails"]["companyName"]
            else:
                company_name = sanjay["included"][1]["name"]

            post_csv = (
                f'"{company_name}","{job_title}","{job_apply_url}","{job_id_url}"'
            )
            print(f"[{job_num}]: {post_csv}")

            with open("./data/linkedin-jobs.csv", "a", encoding="utf-8") as f:
                f.write(f"{post_csv}\n")

            job_num += 1
            time.sleep(args.sleeptime)
